{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.randn(15,6)\n",
    "#torch.sum(a,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from egnn_pytorch import EGNN_Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import gzip\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) *\n",
    "                             (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if embed_dim % 2 == 1:\n",
    "            # Handle odd dimensions by filling the remaining column with cos()\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, :pe[:, 1::2].shape[1]]\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, embed_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor with shape (seq_length, batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class SimpleMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(SimpleMultiheadAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (seq_length, batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        attn_outputs, attn_weights = self.multihead_attn(x, x, x)\n",
    "        return attn_outputs\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, 1, padding=1)\n",
    "            #nn.ReLU(),\n",
    "            #nn.Conv2d(16, 32, 3, padding=1),\n",
    "            #nn.ReLU()\n",
    "        #)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.out  = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x).flatten(1)   # (B, 32)\n",
    "        return self.out(x)\n",
    "\n",
    "       # (B, 1)\n",
    "    \n",
    "model = TinyRegressor()\n",
    "#lin=nn.Linear(6,1)\n",
    "A = PositionalEncoding(1)\n",
    "mha = SimpleMultiheadAttention(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: This function is deprecated. Please call randint(0, 20993 + 1) instead\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:972: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "tensor([[-0.2684]], grad_fn=<AddmmBackward0>) tensor(9.0046e-05, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2549]], grad_fn=<AddmmBackward0>) tensor(1.2545, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2332]], grad_fn=<AddmmBackward0>) tensor(1.7137, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2152]], grad_fn=<AddmmBackward0>) tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2136]], grad_fn=<AddmmBackward0>) tensor(5.3318, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1987]], grad_fn=<AddmmBackward0>) tensor(0.0095, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1905]], grad_fn=<AddmmBackward0>) tensor(0.9942, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1846]], grad_fn=<AddmmBackward0>) tensor(0.1083, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1781]], grad_fn=<AddmmBackward0>) tensor(0.0256, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1705]], grad_fn=<AddmmBackward0>) tensor(1.2344, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1604]], grad_fn=<AddmmBackward0>) tensor(1.4708, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1493]], grad_fn=<AddmmBackward0>) tensor(0.8079, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1367]], grad_fn=<AddmmBackward0>) tensor(0.3217, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1240]], grad_fn=<AddmmBackward0>) tensor(1.6172, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1130]], grad_fn=<AddmmBackward0>) tensor(0.7283, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1008]], grad_fn=<AddmmBackward0>) tensor(0.8719, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0915]], grad_fn=<AddmmBackward0>) tensor(0.0241, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0808]], grad_fn=<AddmmBackward0>) tensor(0.1209, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0728]], grad_fn=<AddmmBackward0>) tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0654]], grad_fn=<AddmmBackward0>) tensor(2.2221, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0575]], grad_fn=<AddmmBackward0>) tensor(0.0913, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0507]], grad_fn=<AddmmBackward0>) tensor(1.2865, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0433]], grad_fn=<AddmmBackward0>) tensor(0.0578, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0347]], grad_fn=<AddmmBackward0>) tensor(0.9677, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0251]], grad_fn=<AddmmBackward0>) tensor(0.7099, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0167]], grad_fn=<AddmmBackward0>) tensor(0.1255, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0069]], grad_fn=<AddmmBackward0>) tensor(1.4498, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0025]], grad_fn=<AddmmBackward0>) tensor(0.0008, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0117]], grad_fn=<AddmmBackward0>) tensor(0.6303, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0213]], grad_fn=<AddmmBackward0>) tensor(0.0506, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0267]], grad_fn=<AddmmBackward0>) tensor(0.2347, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0323]], grad_fn=<AddmmBackward0>) tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0370]], grad_fn=<AddmmBackward0>) tensor(0.4728, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0420]], grad_fn=<AddmmBackward0>) tensor(2.9740, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0425]], grad_fn=<AddmmBackward0>) tensor(1.2363, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0416]], grad_fn=<AddmmBackward0>) tensor(3.1187, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0384]], grad_fn=<AddmmBackward0>) tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0328]], grad_fn=<AddmmBackward0>) tensor(3.2177e-05, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0311]], grad_fn=<AddmmBackward0>) tensor(0.4431, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0286]], grad_fn=<AddmmBackward0>) tensor(0.7728, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0151]], grad_fn=<AddmmBackward0>) tensor(3.2823, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0128]], grad_fn=<AddmmBackward0>) tensor(2.3127, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0064]], grad_fn=<AddmmBackward0>) tensor(0.9528, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0357]], grad_fn=<AddmmBackward0>) tensor(2.0898, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0853]], grad_fn=<AddmmBackward0>) tensor(0.1855, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0896]], grad_fn=<AddmmBackward0>) tensor(1.6264, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1152]], grad_fn=<AddmmBackward0>) tensor(0.0774, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2289]], grad_fn=<AddmmBackward0>) tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1819]], grad_fn=<AddmmBackward0>) tensor(1.6498, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2343]], grad_fn=<AddmmBackward0>) tensor(3.9537, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2244]], grad_fn=<AddmmBackward0>) tensor(0.1603, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3423]], grad_fn=<AddmmBackward0>) tensor(0.0623, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3256]], grad_fn=<AddmmBackward0>) tensor(0.0415, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3161]], grad_fn=<AddmmBackward0>) tensor(0.0225, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3524]], grad_fn=<AddmmBackward0>) tensor(0.0789, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4354]], grad_fn=<AddmmBackward0>) tensor(1.2333, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4258]], grad_fn=<AddmmBackward0>) tensor(0.2940, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4323]], grad_fn=<AddmmBackward0>) tensor(0.0184, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4714]], grad_fn=<AddmmBackward0>) tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5545]], grad_fn=<AddmmBackward0>) tensor(0.8330, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6558]], grad_fn=<AddmmBackward0>) tensor(0.1308, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5308]], grad_fn=<AddmmBackward0>) tensor(0.4776, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5022]], grad_fn=<AddmmBackward0>) tensor(0.0033, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5606]], grad_fn=<AddmmBackward0>) tensor(2.7772, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5750]], grad_fn=<AddmmBackward0>) tensor(1.2799, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6899]], grad_fn=<AddmmBackward0>) tensor(1.6562, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6878]], grad_fn=<AddmmBackward0>) tensor(0.1412, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.7452]], grad_fn=<AddmmBackward0>) tensor(0.0128, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.9305]], grad_fn=<AddmmBackward0>) tensor(0.0248, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.8314]], grad_fn=<AddmmBackward0>) tensor(1.8900, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6841]], grad_fn=<AddmmBackward0>) tensor(0.0037, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.7347]], grad_fn=<AddmmBackward0>) tensor(2.0247, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.1614]], grad_fn=<AddmmBackward0>) tensor(1.5718, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.1618]], grad_fn=<AddmmBackward0>) tensor(0.1571, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.0127]], grad_fn=<AddmmBackward0>) tensor(0.2593, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.9968]], grad_fn=<AddmmBackward0>) tensor(0.3105, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.4100]], grad_fn=<AddmmBackward0>) tensor(0.1057, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.2359]], grad_fn=<AddmmBackward0>) tensor(0.7168, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.2225]], grad_fn=<AddmmBackward0>) tensor(0.7159, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.0266]], grad_fn=<AddmmBackward0>) tensor(1.0910, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.0222]], grad_fn=<AddmmBackward0>) tensor(2.1248, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.9716]], grad_fn=<AddmmBackward0>) tensor(0.4382, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.9708]], grad_fn=<AddmmBackward0>) tensor(1.0061, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6852]], grad_fn=<AddmmBackward0>) tensor(0.4572, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6734]], grad_fn=<AddmmBackward0>) tensor(0.0288, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6269]], grad_fn=<AddmmBackward0>) tensor(0.0152, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5553]], grad_fn=<AddmmBackward0>) tensor(3.1193, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4674]], grad_fn=<AddmmBackward0>) tensor(1.7134, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4436]], grad_fn=<AddmmBackward0>) tensor(0.9154, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4044]], grad_fn=<AddmmBackward0>) tensor(0.5595, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3735]], grad_fn=<AddmmBackward0>) tensor(0.7383, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3371]], grad_fn=<AddmmBackward0>) tensor(0.0728, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3191]], grad_fn=<AddmmBackward0>) tensor(1.0783, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2837]], grad_fn=<AddmmBackward0>) tensor(0.9786, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2545]], grad_fn=<AddmmBackward0>) tensor(0.1090, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2223]], grad_fn=<AddmmBackward0>) tensor(0.1832, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1889]], grad_fn=<AddmmBackward0>) tensor(0.9390, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1600]], grad_fn=<AddmmBackward0>) tensor(3.6381, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1130]], grad_fn=<AddmmBackward0>) tensor(0.0076, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0708]], grad_fn=<AddmmBackward0>) tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0319]], grad_fn=<AddmmBackward0>) tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0094]], grad_fn=<AddmmBackward0>) tensor(0.0066, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0570]], grad_fn=<AddmmBackward0>) tensor(4.6678, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.1036]], grad_fn=<AddmmBackward0>) tensor(0.0589, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.1472]], grad_fn=<AddmmBackward0>) tensor(1.5888, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.1932]], grad_fn=<AddmmBackward0>) tensor(0.1302, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2264]], grad_fn=<AddmmBackward0>) tensor(0.0198, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2660]], grad_fn=<AddmmBackward0>) tensor(0.1716, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3070]], grad_fn=<AddmmBackward0>) tensor(6.3645, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3268]], grad_fn=<AddmmBackward0>) tensor(0.0715, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3376]], grad_fn=<AddmmBackward0>) tensor(0.2569, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3359]], grad_fn=<AddmmBackward0>) tensor(0.6743, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3288]], grad_fn=<AddmmBackward0>) tensor(1.0244, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3076]], grad_fn=<AddmmBackward0>) tensor(0.0027, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2724]], grad_fn=<AddmmBackward0>) tensor(0.0533, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2247]], grad_fn=<AddmmBackward0>) tensor(0.3467, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.1745]], grad_fn=<AddmmBackward0>) tensor(0.4262, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0959]], grad_fn=<AddmmBackward0>) tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0687]], grad_fn=<AddmmBackward0>) tensor(0.8278, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.1452]], grad_fn=<AddmmBackward0>) tensor(3.9665, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2482]], grad_fn=<AddmmBackward0>) tensor(2.7969, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3933]], grad_fn=<AddmmBackward0>) tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.3598]], grad_fn=<AddmmBackward0>) tensor(0.7272, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2082]], grad_fn=<AddmmBackward0>) tensor(0.0253, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2841]], grad_fn=<AddmmBackward0>) tensor(2.2271, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2929]], grad_fn=<AddmmBackward0>) tensor(0.0556, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.4728]], grad_fn=<AddmmBackward0>) tensor(5.9496, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.1918]], grad_fn=<AddmmBackward0>) tensor(1.0343, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.2825]], grad_fn=<AddmmBackward0>) tensor(0.4407, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.1872]], grad_fn=<AddmmBackward0>) tensor(1.1204, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.9740]], grad_fn=<AddmmBackward0>) tensor(2.3519, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.5798]], grad_fn=<AddmmBackward0>) tensor(5.3522, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.2371]], grad_fn=<AddmmBackward0>) tensor(0.0211, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.6896]], grad_fn=<AddmmBackward0>) tensor(0.1145, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.5990]], grad_fn=<AddmmBackward0>) tensor(0.0731, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.4268]], grad_fn=<AddmmBackward0>) tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.4929]], grad_fn=<AddmmBackward0>) tensor(0.3993, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0767]], grad_fn=<AddmmBackward0>) tensor(5.4636, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0053]], grad_fn=<AddmmBackward0>) tensor(0.2614, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5933]], grad_fn=<AddmmBackward0>) tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.9153]], grad_fn=<AddmmBackward0>) tensor(0.2719, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.9932]], grad_fn=<AddmmBackward0>) tensor(1.1874, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4137]], grad_fn=<AddmmBackward0>) tensor(0.0387, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2638]], grad_fn=<AddmmBackward0>) tensor(2.5240, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.1527]], grad_fn=<AddmmBackward0>) tensor(0.0383, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3244]], grad_fn=<AddmmBackward0>) tensor(2.9800, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.5039]], grad_fn=<AddmmBackward0>) tensor(3.2042, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.6556]], grad_fn=<AddmmBackward0>) tensor(0.0575, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.5939]], grad_fn=<AddmmBackward0>) tensor(0.7580, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.4837]], grad_fn=<AddmmBackward0>) tensor(1.3172, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.3646]], grad_fn=<AddmmBackward0>) tensor(3.8118, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2847]], grad_fn=<AddmmBackward0>) tensor(1.5707, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.4124]], grad_fn=<AddmmBackward0>) tensor(0.7940, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.5267]], grad_fn=<AddmmBackward0>) tensor(9.3759, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.8463]], grad_fn=<AddmmBackward0>) tensor(0.7108, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.1477]], grad_fn=<AddmmBackward0>) tensor(3.0950, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.1374]], grad_fn=<AddmmBackward0>) tensor(0.0473, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.1720]], grad_fn=<AddmmBackward0>) tensor(0.0018, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.5788]], grad_fn=<AddmmBackward0>) tensor(1.7812, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.0721]], grad_fn=<AddmmBackward0>) tensor(0.0415, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.6685]], grad_fn=<AddmmBackward0>) tensor(0.2023, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.6230]], grad_fn=<AddmmBackward0>) tensor(0.6524, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.8503]], grad_fn=<AddmmBackward0>) tensor(0.0225, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2582]], grad_fn=<AddmmBackward0>) tensor(0.9720, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2695]], grad_fn=<AddmmBackward0>) tensor(0.2223, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5503]], grad_fn=<AddmmBackward0>) tensor(0.4208, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.2292]], grad_fn=<AddmmBackward0>) tensor(1.5760, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.6829]], grad_fn=<AddmmBackward0>) tensor(0.5992, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[2.1287]], grad_fn=<AddmmBackward0>) tensor(0.0084, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.8934]], grad_fn=<AddmmBackward0>) tensor(0.0469, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.0522]], grad_fn=<AddmmBackward0>) tensor(0.1882, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.0675]], grad_fn=<AddmmBackward0>) tensor(0.0569, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-2.3534]], grad_fn=<AddmmBackward0>) tensor(2.9522, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[3.4824]], grad_fn=<AddmmBackward0>) tensor(0.1230, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.1000]], grad_fn=<AddmmBackward0>) tensor(0.3995, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4371]], grad_fn=<AddmmBackward0>) tensor(0.9914, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.6504]], grad_fn=<AddmmBackward0>) tensor(5.4220, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.3424]], grad_fn=<AddmmBackward0>) tensor(6.6132, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.4427]], grad_fn=<AddmmBackward0>) tensor(4.9047, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5180]], grad_fn=<AddmmBackward0>) tensor(0.0428, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.8537]], grad_fn=<AddmmBackward0>) tensor(0.0547, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-2.0320]], grad_fn=<AddmmBackward0>) tensor(3.7318, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-3.2868]], grad_fn=<AddmmBackward0>) tensor(6.0496, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-6.1546]], grad_fn=<AddmmBackward0>) tensor(2.2353, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.1536]], grad_fn=<AddmmBackward0>) tensor(3.6053, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-3.5520]], grad_fn=<AddmmBackward0>) tensor(2.4932, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-4.3136]], grad_fn=<AddmmBackward0>) tensor(3.7738, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-4.2534]], grad_fn=<AddmmBackward0>) tensor(3.5556, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.3433]], grad_fn=<AddmmBackward0>) tensor(1.4218, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.5793]], grad_fn=<AddmmBackward0>) tensor(0.2282, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.2057]], grad_fn=<AddmmBackward0>) tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.5595]], grad_fn=<AddmmBackward0>) tensor(0.3739, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.0004]], grad_fn=<AddmmBackward0>) tensor(1.5155, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.3668]], grad_fn=<AddmmBackward0>) tensor(0.9267, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[2.2013]], grad_fn=<AddmmBackward0>) tensor(0.5333, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[3.0451]], grad_fn=<AddmmBackward0>) tensor(3.1183, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[4.4905]], grad_fn=<AddmmBackward0>) tensor(6.0325, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[6.7571]], grad_fn=<AddmmBackward0>) tensor(4.7641, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[8.3370]], grad_fn=<AddmmBackward0>) tensor(7.6987, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[9.2234]], grad_fn=<AddmmBackward0>) tensor(8.5109, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[9.9423]], grad_fn=<AddmmBackward0>) tensor(2.4027, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[7.9203]], grad_fn=<AddmmBackward0>) tensor(10.2695, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[7.5868]], grad_fn=<AddmmBackward0>) tensor(6.2681, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[6.5050]], grad_fn=<AddmmBackward0>) tensor(5.7052, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[4.8990]], grad_fn=<AddmmBackward0>) tensor(3.8484, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[5.2984]], grad_fn=<AddmmBackward0>) tensor(3.8521, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.0120]], grad_fn=<AddmmBackward0>) tensor(0.6644, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.8300]], grad_fn=<AddmmBackward0>) tensor(0.7513, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2918]], grad_fn=<AddmmBackward0>) tensor(0.0014, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.9215]], grad_fn=<AddmmBackward0>) tensor(0.8358, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.5408]], grad_fn=<AddmmBackward0>) tensor(0.2852, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.4233]], grad_fn=<AddmmBackward0>) tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-8.4982]], grad_fn=<AddmmBackward0>) tensor(3.8406, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-10.1770]], grad_fn=<AddmmBackward0>) tensor(9.3300, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-10.5063]], grad_fn=<AddmmBackward0>) tensor(9.7525, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-6.7026]], grad_fn=<AddmmBackward0>) tensor(6.1444, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-0.2575]], grad_fn=<AddmmBackward0>) tensor(0.5813, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[5.3161]], grad_fn=<AddmmBackward0>) tensor(6.8093, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[6.5798]], grad_fn=<AddmmBackward0>) tensor(6.4660, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-6.3782]], grad_fn=<AddmmBackward0>) tensor(6.4652, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-32.9004]], grad_fn=<AddmmBackward0>) tensor(39.5459, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-39.6200]], grad_fn=<AddmmBackward0>) tensor(38.5399, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-27.1021]], grad_fn=<AddmmBackward0>) tensor(26.3948, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[5.3257]], grad_fn=<AddmmBackward0>) tensor(5.7070, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[26.1333]], grad_fn=<AddmmBackward0>) tensor(26.0535, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[56.5363]], grad_fn=<AddmmBackward0>) tensor(57.2746, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[31.7724]], grad_fn=<AddmmBackward0>) tensor(28.7856, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[14.8946]], grad_fn=<AddmmBackward0>) tensor(14.4077, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-46.1730]], grad_fn=<AddmmBackward0>) tensor(49.8401, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-116.3790]], grad_fn=<AddmmBackward0>) tensor(115.6872, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-114.7060]], grad_fn=<AddmmBackward0>) tensor(113.8549, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-82.0381]], grad_fn=<AddmmBackward0>) tensor(78.6239, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-103.5860]], grad_fn=<AddmmBackward0>) tensor(103.7507, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-62.0501]], grad_fn=<AddmmBackward0>) tensor(62.2909, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-31.3118]], grad_fn=<AddmmBackward0>) tensor(30.5818, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-13.0851]], grad_fn=<AddmmBackward0>) tensor(12.3447, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1.0547]], grad_fn=<AddmmBackward0>) tensor(0.0331, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.5320]], grad_fn=<AddmmBackward0>) tensor(7.7366, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[0.5452]], grad_fn=<AddmmBackward0>) tensor(0.5638, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[5.2743]], grad_fn=<AddmmBackward0>) tensor(4.5749, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[6.7067]], grad_fn=<AddmmBackward0>) tensor(6.0378, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[17.5801]], grad_fn=<AddmmBackward0>) tensor(19.1140, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[36.0605]], grad_fn=<AddmmBackward0>) tensor(33.1939, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[100.4813]], grad_fn=<AddmmBackward0>) tensor(100.2954, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[193.6961]], grad_fn=<AddmmBackward0>) tensor(193.6968, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[190.5446]], grad_fn=<AddmmBackward0>) tensor(190.2371, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[411.6521]], grad_fn=<AddmmBackward0>) tensor(411.3905, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[55.5945]], grad_fn=<AddmmBackward0>) tensor(56.4397, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[104.5679]], grad_fn=<AddmmBackward0>) tensor(98.6991, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[165.9933]], grad_fn=<AddmmBackward0>) tensor(164.8768, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[360.7812]], grad_fn=<AddmmBackward0>) tensor(361.9765, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1.6915]], grad_fn=<AddmmBackward0>) tensor(1.1245, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[46.8819]], grad_fn=<AddmmBackward0>) tensor(46.5390, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-102.7613]], grad_fn=<AddmmBackward0>) tensor(99.0073, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-275.6089]], grad_fn=<AddmmBackward0>) tensor(275.1181, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-544.2072]], grad_fn=<AddmmBackward0>) tensor(541.2364, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-446.2855]], grad_fn=<AddmmBackward0>) tensor(443.8595, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-616.2001]], grad_fn=<AddmmBackward0>) tensor(617.3105, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-676.6192]], grad_fn=<AddmmBackward0>) tensor(676.5203, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-387.3768]], grad_fn=<AddmmBackward0>) tensor(386.4070, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-231.4552]], grad_fn=<AddmmBackward0>) tensor(230.7159, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-38.5250]], grad_fn=<AddmmBackward0>) tensor(36.9092, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[272.0665]], grad_fn=<AddmmBackward0>) tensor(269.6954, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[258.5108]], grad_fn=<AddmmBackward0>) tensor(252.1561, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[408.7774]], grad_fn=<AddmmBackward0>) tensor(408.6971, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1855.1174]], grad_fn=<AddmmBackward0>) tensor(1855.6025, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1374.2087]], grad_fn=<AddmmBackward0>) tensor(1372.8491, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[134.9375]], grad_fn=<AddmmBackward0>) tensor(133.9325, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-97.1150]], grad_fn=<AddmmBackward0>) tensor(95.7710, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[10.6143]], grad_fn=<AddmmBackward0>) tensor(10.5819, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-9.4650]], grad_fn=<AddmmBackward0>) tensor(8.7900, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[2.4202]], grad_fn=<AddmmBackward0>) tensor(1.9406, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[4.8059]], grad_fn=<AddmmBackward0>) tensor(6.0432, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[16.7673]], grad_fn=<AddmmBackward0>) tensor(17.6729, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[35.0670]], grad_fn=<AddmmBackward0>) tensor(36.3591, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[39.7256]], grad_fn=<AddmmBackward0>) tensor(39.4158, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[25.9165]], grad_fn=<AddmmBackward0>) tensor(25.5704, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[3.7590]], grad_fn=<AddmmBackward0>) tensor(3.5195, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-15.6457]], grad_fn=<AddmmBackward0>) tensor(15.0478, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-10.0180]], grad_fn=<AddmmBackward0>) tensor(9.1808, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[49.0975]], grad_fn=<AddmmBackward0>) tensor(48.7350, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[177.2907]], grad_fn=<AddmmBackward0>) tensor(177.6684, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[440.0538]], grad_fn=<AddmmBackward0>) tensor(439.4143, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[920.9750]], grad_fn=<AddmmBackward0>) tensor(919.0820, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[971.6320]], grad_fn=<AddmmBackward0>) tensor(971.0906, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1701.5449]], grad_fn=<AddmmBackward0>) tensor(1700.7205, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[2039.8853]], grad_fn=<AddmmBackward0>) tensor(2039.9750, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1431.6782]], grad_fn=<AddmmBackward0>) tensor(1431.0836, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[987.6728]], grad_fn=<AddmmBackward0>) tensor(981.6659, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[282.4153]], grad_fn=<AddmmBackward0>) tensor(281.8647, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-938.4164]], grad_fn=<AddmmBackward0>) tensor(938.2438, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-2621.5427]], grad_fn=<AddmmBackward0>) tensor(2621.0818, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-5176.8975]], grad_fn=<AddmmBackward0>) tensor(5175.5371, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-16343.3662]], grad_fn=<AddmmBackward0>) tensor(16343.2471, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-2934.4902]], grad_fn=<AddmmBackward0>) tensor(2935.4468, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-941.4481]], grad_fn=<AddmmBackward0>) tensor(939.2617, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-497.1685]], grad_fn=<AddmmBackward0>) tensor(496.9430, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[327.2139]], grad_fn=<AddmmBackward0>) tensor(329.5819, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[858.1744]], grad_fn=<AddmmBackward0>) tensor(856.9091, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[213.5705]], grad_fn=<AddmmBackward0>) tensor(212.6074, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1167.9713]], grad_fn=<AddmmBackward0>) tensor(1166.8110, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-2263.8240]], grad_fn=<AddmmBackward0>) tensor(2263.0474, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-3270.2996]], grad_fn=<AddmmBackward0>) tensor(3270.6533, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-3297.8767]], grad_fn=<AddmmBackward0>) tensor(3299.1165, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-2290.3159]], grad_fn=<AddmmBackward0>) tensor(2290.4043, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-1168.4036]], grad_fn=<AddmmBackward0>) tensor(1166.1741, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[453.4073]], grad_fn=<AddmmBackward0>) tensor(453.4148, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[2432.1282]], grad_fn=<AddmmBackward0>) tensor(2428.8794, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[3701.4663]], grad_fn=<AddmmBackward0>) tensor(3701.7725, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[4191.4707]], grad_fn=<AddmmBackward0>) tensor(4191.8276, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[3494.5208]], grad_fn=<AddmmBackward0>) tensor(3495.3398, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[245.2496]], grad_fn=<AddmmBackward0>) tensor(246.1207, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-4585.0854]], grad_fn=<AddmmBackward0>) tensor(4589.4409, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-13223.7969]], grad_fn=<AddmmBackward0>) tensor(13223.9590, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-24098.1270]], grad_fn=<AddmmBackward0>) tensor(24097.0820, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-28505.4395]], grad_fn=<AddmmBackward0>) tensor(28504.6602, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-30246.6016]], grad_fn=<AddmmBackward0>) tensor(30251.7129, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-21076.0703]], grad_fn=<AddmmBackward0>) tensor(21077.0684, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[-11358.3545]], grad_fn=<AddmmBackward0>) tensor(11357.2285, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[1310.9392]], grad_fn=<AddmmBackward0>) tensor(1310.3573, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[11847.8750]], grad_fn=<AddmmBackward0>) tensor(11844.0898, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[14190.0283]], grad_fn=<AddmmBackward0>) tensor(14188.4219, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[34573.8477]], grad_fn=<AddmmBackward0>) tensor(34574.1250, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[36480.8281]], grad_fn=<AddmmBackward0>) tensor(36480.9961, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[33849.2969]], grad_fn=<AddmmBackward0>) tensor(33848.5273, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[28544.3438]], grad_fn=<AddmmBackward0>) tensor(28543.5938, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[15810.5459]], grad_fn=<AddmmBackward0>) tensor(15809.6123, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[6170.3628]], grad_fn=<AddmmBackward0>) tensor(6170.2568, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[380.8966]], grad_fn=<AddmmBackward0>) tensor(382.7558, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[2134.3499]], grad_fn=<AddmmBackward0>) tensor(2134.4922, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[14542.5811]], grad_fn=<AddmmBackward0>) tensor(14542.6289, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[41280.1836]], grad_fn=<AddmmBackward0>) tensor(41280.2891, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[87583.9922]], grad_fn=<AddmmBackward0>) tensor(87583.2500, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[141318.2500]], grad_fn=<AddmmBackward0>) tensor(141315.6719, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[188266.5312]], grad_fn=<AddmmBackward0>) tensor(188263.5625, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[232633.0938]], grad_fn=<AddmmBackward0>) tensor(232631.5000, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[2.2367e+15]], grad_fn=<AddmmBackward0>) tensor(2.2367e+15, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>) tensor(nan, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-2175125b8f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net=EGNN_Network(dim=2,\n",
    "    depth=6,\n",
    "    num_positions=500,\n",
    "    num_tokens=78,\n",
    "    num_edge_tokens=3,\n",
    "    global_linear_attn_every=1,\n",
    "    global_linear_attn_dim_head=8,\n",
    "    num_global_tokens=2,\n",
    "    adj_dim=3,\n",
    "    fourier_features=2,\n",
    "    m_dim=2,\n",
    "    dropout=0.05)\n",
    "\n",
    "#lin=nn.Linear(6,1)\n",
    "A = PositionalEncoding(1)\n",
    "mha = SimpleMultiheadAttention(2,1) #6 is the dim\n",
    "tlosses, losses=[],[]\n",
    "paths=np.char.array(glob.glob(\"/home/jrhoernschemeyer/Desktop/data_prep/inputs/*.npz\"))\n",
    "val=paths[np.random.random_integers(low=0,high=len(paths)-2,size=np.int(0.4*len(paths)))]\n",
    "train=np.char.array(list(set(paths).difference(set(val))))\n",
    "#np.savez_compressed(\"/home/jrhoernschemeyer/Desktop/data_prep/split.npz\",val=val, train=train)\n",
    "\n",
    "optimizer= torch.optim.Adam(list(net.parameters()) + list(mha.parameters()) + list(model.parameters()), lr=.01, weight_decay=0.01)\n",
    "criterion = nn.HuberLoss()\n",
    "to=time.time()\n",
    "for i in range(1):\n",
    "    #try:\n",
    "    \n",
    "    train = list(np.array(train)[np.random.permutation(len(train))])\n",
    "    print(\"epoch\",i)\n",
    "    for path in train:\n",
    "        losses=[]\n",
    "        pdb=np.char.encode(path[-8:-4])\n",
    "        mha.train()\n",
    "        net.train()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        a=np.load(path,allow_pickle=True)\n",
    "        zs,xs,targets=a[\"z\"],a[\"pos\"],a[\"pks\"]\n",
    "        n = zs.shape[0]\n",
    "        #shuffle\n",
    "        idx = np.random.permutation(n)\n",
    "        zs,xs,targets=zs[idx],xs[idx],targets[idx]\n",
    "        \n",
    "        for z,x,y in zip(zs,xs,targets):\n",
    "\n",
    "            x=torch.tensor(x).unsqueeze(0)\n",
    "            z=torch.tensor(z[0].astype(np.int32)).unsqueeze(0)\n",
    "\n",
    "            #x=torch.tensor(list(x)).unsqueeze(0)\n",
    "            #z=torch.tensor(list(z),dtype=torch.int32)\n",
    "            #remove H\n",
    "            #mask = (z != 1)\n",
    "            #z = z[mask]\n",
    "            #x = x[mask]\n",
    "\n",
    "            out=net(z,x)\n",
    "            x = mha(A(out[0][0]))          #  shape: (H, W, C) = (9, 9, 6)\n",
    "\n",
    "            x = x.permute(2, 0, 1).unsqueeze(0)   #  (B, C, H, W) = (1, 6, 9, 9)\n",
    "\n",
    "            pk = model(x)        \n",
    "            #out=mha(A(out[0][0]))\n",
    "            #out.permute(2, 0, 1).unsqueeze(0) \n",
    "            #pk=model(out)\n",
    "            #print(\"why\")\n",
    "            loss = criterion(pk,torch.tensor(y))\n",
    "            tlosses.append(np.round(loss.item(),3))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(pk,loss)\n",
    "\n",
    "            #pk=torch.sum(out[0],dim=1)/(torch.max(out[0]))\n",
    "            #out=lin(pk)\n",
    "\n",
    "    print((\"validating. last loss\", loss, time.time()-to)/60,\"minutes\")\n",
    "    for path in val:\n",
    "        mha.eval()\n",
    "        net.eval()\n",
    "        model.eval()\n",
    "\n",
    "        a=np.load(path,allow_pickle=True)\n",
    "        zs,xs,targets=a[\"z\"],a[\"pos\"],a[\"pks\"]\n",
    "        #n = zs.shape[0]\n",
    "        #shuffle\n",
    "        #idx = np.random.permutation(n)\n",
    "        #zs,xs,targets=zs[idx],xs[idx],targets[idx]\n",
    "        \n",
    "        for z,x,y in zip(zs,xs,targets):\n",
    "            \n",
    "            x=torch.tensor(x).unsqueeze(0)\n",
    "            z=torch.tensor(z[0].astype(np.int32)).unsqueeze(0)\n",
    "            \n",
    "            mask= z != 1\n",
    "            z = z[mask]\n",
    "            x = x[mask]\n",
    "            oout=net(z,x)\n",
    "            x = mha(A(out[0][0]))          #  shape: (H, W, C) = (9, 9, 6)\n",
    "\n",
    "            x = x.permute(2, 0, 1).unsqueeze(0)   #  (B, C, H, W) = (1, 6, 9, 9)\n",
    "\n",
    "            pk = model(x)\n",
    "            loss = criterion(out,torch.tensor(y))\n",
    "            losses.append(np.round(loss.item(),3))\n",
    "            out=net(z,x)\n",
    "            x = mha(A(out[0][0]))          #  shape: (H, W, C) = (9, 9, 6)\n",
    "\n",
    "            x = x.permute(2, 0, 1).unsqueeze(0)   #  (B, C, H, W) = (1, 6, 9, 9)\n",
    "\n",
    "            pk = model(x)        \n",
    "            #out=mha(A(out[0][0]))\n",
    "            #out.permute(2, 0, 1).unsqueeze(0) \n",
    "            #pk=model(out)\n",
    "            #print(\"why\")\n",
    "            loss = criterion(pk,torch.tensor(y))\n",
    "            losses.append(np.round(loss.item(),3))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(pk,loss)\n",
    "\n",
    "    #\n",
    "                    \n",
    "    #except Exception as e:\n",
    "        #print(\"exception\",e,pdb,z)\n",
    "        #continue    \n",
    "        #with gzip.open(\"/home/jrhoernschemeyer/Desktop/data_prep/valresultsfinal.gz\",\"a\") as f:\n",
    "            #f.write(np.char.encode(str(np.round(np.mean(losses).item(),3))))\n",
    "            #f.write(b\" \")\n",
    "            #f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 6, 6, 8, 6, 6, 6, 6, 7], dtype=torch.int32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EGNN_Network(\n",
       "  (token_emb): Embedding(78, 2)\n",
       "  (pos_emb): Embedding(500, 2)\n",
       "  (edge_emb): Embedding(3, 0)\n",
       "  (layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): GlobalLinearAttention(\n",
       "        (norm_seq): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_queries): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=8, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EGNN(\n",
       "        (edge_mlp): Sequential(\n",
       "          (0): Linear(in_features=9, out_features=18, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=18, out_features=2, bias=True)\n",
       "          (4): SiLU()\n",
       "        )\n",
       "        (node_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (coors_norm): Identity()\n",
       "        (node_mlp): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=4, out_features=2, bias=True)\n",
       "        )\n",
       "        (coors_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=8, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): GlobalLinearAttention(\n",
       "        (norm_seq): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_queries): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=8, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EGNN(\n",
       "        (edge_mlp): Sequential(\n",
       "          (0): Linear(in_features=9, out_features=18, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=18, out_features=2, bias=True)\n",
       "          (4): SiLU()\n",
       "        )\n",
       "        (node_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (coors_norm): Identity()\n",
       "        (node_mlp): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=4, out_features=2, bias=True)\n",
       "        )\n",
       "        (coors_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=8, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): GlobalLinearAttention(\n",
       "        (norm_seq): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_queries): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=8, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EGNN(\n",
       "        (edge_mlp): Sequential(\n",
       "          (0): Linear(in_features=9, out_features=18, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=18, out_features=2, bias=True)\n",
       "          (4): SiLU()\n",
       "        )\n",
       "        (node_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (coors_norm): Identity()\n",
       "        (node_mlp): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=4, out_features=2, bias=True)\n",
       "        )\n",
       "        (coors_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=8, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): GlobalLinearAttention(\n",
       "        (norm_seq): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_queries): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=8, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EGNN(\n",
       "        (edge_mlp): Sequential(\n",
       "          (0): Linear(in_features=9, out_features=18, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=18, out_features=2, bias=True)\n",
       "          (4): SiLU()\n",
       "        )\n",
       "        (node_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (coors_norm): Identity()\n",
       "        (node_mlp): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=4, out_features=2, bias=True)\n",
       "        )\n",
       "        (coors_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=8, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): ModuleList(\n",
       "      (0): GlobalLinearAttention(\n",
       "        (norm_seq): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_queries): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=8, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EGNN(\n",
       "        (edge_mlp): Sequential(\n",
       "          (0): Linear(in_features=9, out_features=18, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=18, out_features=2, bias=True)\n",
       "          (4): SiLU()\n",
       "        )\n",
       "        (node_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (coors_norm): Identity()\n",
       "        (node_mlp): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=4, out_features=2, bias=True)\n",
       "        )\n",
       "        (coors_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=8, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): ModuleList(\n",
       "      (0): GlobalLinearAttention(\n",
       "        (norm_seq): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_queries): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=2, out_features=64, bias=False)\n",
       "          (to_kv): Linear(in_features=2, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (2): GELU()\n",
       "          (3): Linear(in_features=8, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EGNN(\n",
       "        (edge_mlp): Sequential(\n",
       "          (0): Linear(in_features=9, out_features=18, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=18, out_features=2, bias=True)\n",
       "          (4): SiLU()\n",
       "        )\n",
       "        (node_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        (coors_norm): Identity()\n",
       "        (node_mlp): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=4, out_features=2, bias=True)\n",
       "        )\n",
       "        (coors_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "          (1): Dropout(p=0.05, inplace=False)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=8, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name\t\t\t\t\t\t\tNumber of Parameters\n",
      "====================================================================================================\n",
      "\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "Embedding(78, 6)\t\t\t480\n",
      "\n",
      "Embedding(500, 6)\t\t\t3000\n",
      "\n",
      "Embedding(2, 0)\t\t\t12\n",
      "\n",
      "ModuleList(\n",
      "  (0): ModuleList(\n",
      "    (0): GlobalLinearAttention(\n",
      "      (norm_seq): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_queries): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn1): Attention(\n",
      "        (to_q): Linear(in_features=6, out_features=48, bias=False)\n",
      "        (to_kv): Linear(in_features=6, out_features=96, bias=False)\n",
      "        (to_out): Linear(in_features=48, out_features=6, bias=True)\n",
      "      )\n",
      "      (attn2): Attention(\n",
      "        (to_q): Linear(in_features=6, out_features=48, bias=False)\n",
      "        (to_kv): Linear(in_features=6, out_features=96, bias=False)\n",
      "        (to_out): Linear(in_features=48, out_features=6, bias=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=6, out_features=24, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=24, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): EGNN(\n",
      "      (edge_mlp): Sequential(\n",
      "        (0): Linear(in_features=23, out_features=46, bias=True)\n",
      "        (1): Dropout(p=0.3, inplace=False)\n",
      "        (2): SiLU()\n",
      "        (3): Linear(in_features=46, out_features=16, bias=True)\n",
      "        (4): SiLU()\n",
      "      )\n",
      "      (node_norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (coors_norm): Identity()\n",
      "      (node_mlp): Sequential(\n",
      "        (0): Linear(in_features=22, out_features=12, bias=True)\n",
      "        (1): Dropout(p=0.3, inplace=False)\n",
      "        (2): SiLU()\n",
      "        (3): Linear(in_features=12, out_features=6, bias=True)\n",
      "      )\n",
      "      (coors_mlp): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "        (1): Dropout(p=0.3, inplace=False)\n",
      "        (2): SiLU()\n",
      "        (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ModuleList(\n",
      "    (0): GlobalLinearAttention(\n",
      "      (norm_seq): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_queries): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn1): Attention(\n",
      "        (to_q): Linear(in_features=6, out_features=48, bias=False)\n",
      "        (to_kv): Linear(in_features=6, out_features=96, bias=False)\n",
      "        (to_out): Linear(in_features=48, out_features=6, bias=True)\n",
      "      )\n",
      "      (attn2): Attention(\n",
      "        (to_q): Linear(in_features=6, out_features=48, bias=False)\n",
      "        (to_kv): Linear(in_features=6, out_features=96, bias=False)\n",
      "        (to_out): Linear(in_features=48, out_features=6, bias=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=6, out_features=24, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=24, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): EGNN(\n",
      "      (edge_mlp): Sequential(\n",
      "        (0): Linear(in_features=23, out_features=46, bias=True)\n",
      "        (1): Dropout(p=0.3, inplace=False)\n",
      "        (2): SiLU()\n",
      "        (3): Linear(in_features=46, out_features=16, bias=True)\n",
      "        (4): SiLU()\n",
      "      )\n",
      "      (node_norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "      (coors_norm): Identity()\n",
      "      (node_mlp): Sequential(\n",
      "        (0): Linear(in_features=22, out_features=12, bias=True)\n",
      "        (1): Dropout(p=0.3, inplace=False)\n",
      "        (2): SiLU()\n",
      "        (3): Linear(in_features=12, out_features=6, bias=True)\n",
      "      )\n",
      "      (coors_mlp): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "        (1): Dropout(p=0.3, inplace=False)\n",
      "        (2): SiLU()\n",
      "        (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\t\t\t12\n",
      "====================================================================================================\n",
      "Total Params:3504\n"
     ]
    }
   ],
   "source": [
    "def model_summary(model):\n",
    "  print(\"model_summary\")\n",
    "  print()\n",
    "  print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
    "  print(\"=\"*100)\n",
    "  model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
    "  layer_name = [child for child in model.children()]\n",
    "  j = 0\n",
    "  total_params = 0\n",
    "  print(\"\\t\"*10)\n",
    "  for i in layer_name:\n",
    "    print()\n",
    "    param = 0\n",
    "    try:\n",
    "      bias = (i.bias is not None)\n",
    "    except:\n",
    "      bias = False  \n",
    "    if not bias:\n",
    "      param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
    "      j = j+2\n",
    "    else:\n",
    "      param =model_parameters[j].numel()\n",
    "      j = j+1\n",
    "    print(str(i)+\"\\t\"*3+str(param))\n",
    "    total_params+=param\n",
    "  print(\"=\"*100)\n",
    "  print(f\"Total Params:{total_params}\")       \n",
    "\n",
    "model_summary(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51195"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=EGNN_Network(dim=6,\n",
    "    depth=10,\n",
    "    num_positions=50,\n",
    "    num_tokens=78,\n",
    "    num_edge_tokens=2,\n",
    "    global_linear_attn_every=1,\n",
    "    global_linear_attn_dim_head=6,\n",
    "    num_global_tokens=10,\n",
    "    adj_dim=3,\n",
    "    m_dim=10,\n",
    "    fourier_features=5,\n",
    "    dropout=0.3)\n",
    "sum(stuff.numel() for stuff in list(net.parameters()) + list(mha.parameters()) + list(model.parameters()) if stuff.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name\t\t\t\t\t\t\tNumber of Parameters\n",
      "====================================================================================================\n",
      "\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "Conv2d(6, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\t\t\t6\n",
      "\n",
      "AdaptiveAvgPool2d(output_size=(1, 1))\t\t\t2\n",
      "\n",
      "Linear(in_features=1, out_features=1, bias=True)\t\t\t1\n",
      "====================================================================================================\n",
      "Total Params:9\n"
     ]
    }
   ],
   "source": [
    "model_summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name\t\t\t\t\t\t\tNumber of Parameters\n",
      "====================================================================================================\n",
      "\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
      ")\t\t\t126\n",
      "====================================================================================================\n",
      "Total Params:126\n"
     ]
    }
   ],
   "source": [
    "model_summary(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [1, 6, 1, 1], but got 3-dimensional input of size [9, 9, 6] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-26f64645f423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-6589a0594371>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (B, 32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;31m#self.out(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [1, 6, 1, 1], but got 3-dimensional input of size [9, 9, 6] instead"
     ]
    }
   ],
   "source": [
    "out=mha(A(out[0][0]))\n",
    "out.permute(2, 0, 1).unsqueeze(0) \n",
    "pk=model(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = SimpleMultiheadAttention(6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 9, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.sum(lin(mha(A(out[0][0]))),dim=0).detach().numpy())\n",
    "lin2=nn.Linear(1,1)\n",
    "lin2(lin(mha(A(out[0][0])))).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 6])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mha(A(out[0][0]))          #  shape: (H, W, C) = (9, 9, 6)\n",
    "\n",
    "x = x.permute(2, 0, 1).unsqueeze(0)   #  (B, C, H, W) = (1, 6, 9, 9)\n",
    "\n",
    "pred = model(x)             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0424]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73a5718898>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(torch.mean(out[0][0],dim=-1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73a57ae518>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.mean(torch.mean(A(out[0][0]),dim=-1),dim=0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z\n",
    "x\n",
    "mask = (z != 1)\n",
    "z = z[mask]\n",
    "x = x[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.5851, -0.6893,  1.4510, -2.5218,  4.2458, -2.3039],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(out[0][0],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 8], dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(z[0].astype(np.int32))\n",
    "(z != 1)\n",
    "                #z = z[mask]\n",
    "                #x = x[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5448], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=A(out[0][0].unsqueeze(2))\n",
    "c=mha(b)\n",
    "d=torch.sum(mha(c),dim=-1)\n",
    "e=lin(d)\n",
    "torch.sum(e)/max(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0].unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:74: DeprecationWarning: This function is deprecated. Please call randint(0, 20993 + 1) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "2\n",
      "2\n",
      "2\n",
      "0.17205572923024495 minutes\n",
      "epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b502b75d1406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m#except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from egnn_pytorch import EGNN_Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import gzip\n",
    "import math\n",
    "with gzip.open(\"/home/jrhoernschemeyer/Desktop/data_prep/valresults_final.gz\",\"wb\") as f:\n",
    "    #f.write(np.char.encode(str(np.round(np.mean(losses).item(),3))))\n",
    "    f.write(b\"0.00\")\n",
    "    f.close()\n",
    "with gzip.open(\"/home/jrhoernschemeyer/Desktop/data_prep/trainresults_final.gz\",\"wb\") as f:\n",
    "    #f.write(np.char.encode(str(np.round(np.mean(losses).item(),3))))\n",
    "    f.write(b\"0.00\")\n",
    "    f.close()\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) *\n",
    "                             (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if embed_dim % 2 == 1:\n",
    "            # Handle odd dimensions by filling the remaining column with cos()\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, :pe[:, 1::2].shape[1]]\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, embed_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor with shape (seq_length, batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class SimpleMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(SimpleMultiheadAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (seq_length, batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        attn_outputs, attn_weights = self.multihead_attn(x, x, x)\n",
    "        return attn_outputs\n",
    "\n",
    "\n",
    "net=EGNN_Network(dim=6,\n",
    "    depth=2,\n",
    "    num_positions=500,\n",
    "    num_tokens=200,\n",
    "    num_nearest_neighbors=2, \n",
    "    num_edge_tokens=2,\n",
    "    global_linear_attn_every=1,\n",
    "    global_linear_attn_dim_head=6,\n",
    "    num_global_tokens=2,\n",
    "    adj_dim=3,\n",
    "    fourier_features=5,\n",
    "    m_dim=10,\n",
    "    dropout=0.3)\n",
    "\n",
    "lin=nn.Linear(6,1)\n",
    "A = PositionalEncoding(22)\n",
    "mha = SimpleMultiheadAttention(22,1)\n",
    "losses=[]\n",
    "paths=np.char.array(glob.glob(\"/home/jrhoernschemeyer/Desktop/data_prep/inputs/*.npz\"))\n",
    "val=paths[np.random.random_integers(low=0,high=len(paths)-2,size=np.int(0.4*len(paths)))][0:3]\n",
    "train=np.char.array(list(set(paths).difference(set(val))))[0:3]\n",
    "np.savez_compressed(\"/home/jrhoernschemeyer/Desktop/data_prep/split.npz\",val=val, train=train)\n",
    "\n",
    "optimizer= torch.optim.Adam(list(net.parameters()) + list(mha.parameters()) + list(lin.parameters()), lr=.01, weight_decay=0.01)\n",
    "criterion = nn.HuberLoss()\n",
    "to=time.time()\n",
    "\n",
    "for i in range(25):\n",
    "        train = list(np.array(train)[np.random.permutation(len(train))])\n",
    "        print(\"epoch\",i)\n",
    "        for path in train:\n",
    "            losses=[]\n",
    "            pdb=np.char.encode(path[-8:-4])\n",
    "            mha.train()\n",
    "            net.train()\n",
    "            lin.train()\n",
    "            optimizer.zero_grad()\n",
    "            a=np.load(path,allow_pickle=True)\n",
    "            zs,xs,targets=a[\"z\"],a[\"pos\"],a[\"pks\"]\n",
    "            n = zs.shape[0]\n",
    "            #shuffle\n",
    "            idx = np.random.permutation(n)\n",
    "            zs,xs,targets=zs[idx],xs[idx],targets[idx]\n",
    "            \n",
    "            for z,x,y in zip(zs,xs,targets):\n",
    "                x=torch.tensor(list(x)).unsqueeze(0)\n",
    "                z=torch.tensor(list(z),dtype=torch.int32)\n",
    "                #remove H\n",
    "                #mask = (z != 1)\n",
    "                #z = z[mask]\n",
    "                #x = x[mask]\n",
    "\n",
    "                try:\n",
    "                    out=net(z,x)[0][0].unsqueeze(2)\n",
    "                    out=mha(A(out))\n",
    "                    out=torch.sum(out,dim=-1)\n",
    "                    out=lin(out)\n",
    "                    out=torch.sum(out)\n",
    "\n",
    "                    #y=torch.sum(out[0],dim=1)/(torch.max(out[0]))\n",
    "                    \n",
    "                    loss = criterion(out,torch.tensor(y))\n",
    "                    losses.append(np.round(loss.item(),3))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                #except:\n",
    "                except Exception as e:\n",
    "                    print(\"exception\",e,pdb,z)\n",
    "                    continue\n",
    "                with gzip.open(\"/home/jrhoernschemeyer/Desktop/data_prep/trainresults_final.gz\",\"a\") as f:\n",
    "                    f.write(np.char.encode(str(np.round(np.mean(losses),3))))\n",
    "                    f.write(b\" \")\n",
    "                    f.close()\n",
    "                \n",
    "        for path in val:\n",
    "            mha.eval()\n",
    "            net.eval()\n",
    "            lin.eval()\n",
    "            print(2)\n",
    "\n",
    "            a=np.load(path,allow_pickle=True)\n",
    "            zs,xs,targets=a[\"z\"],a[\"pos\"],a[\"pks\"]\n",
    "            #n = zs.shape[0]\n",
    "            #shuffle\n",
    "            #idx = np.random.permutation(n)\n",
    "            #zs,xs,targets=zs[idx],xs[idx],targets[idx]\n",
    "            \n",
    "            for z,x,y in zip(zs,xs,targets):\n",
    "                x=torch.tensor(list(x)).unsqueeze(0)\n",
    "                z=torch.tensor(list(z),dtype=torch.int32)\n",
    "                try:\n",
    "                    out=net(z,x)[0][0].unsqueeze(2)\n",
    "                    out=mha(A(out))\n",
    "                    out=torch.sum(out,dim=-1)\n",
    "                    out=lin(out)\n",
    "                    out=torch.sum(out)\n",
    "                    loss = criterion(out,torch.tensor(y))\n",
    "                    losses.append(np.round(loss.item(),3))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(\"exception\",e,pdb,z)\n",
    "                    continue    \n",
    "            with gzip.open(\"/home/jrhoernschemeyer/Desktop/data_prep/valresultsfinal.gz\",\"a\") as f:\n",
    "                f.write(np.char.encode(str(np.round(np.mean(losses).item(),3))))\n",
    "                f.write(b\" \")\n",
    "                f.close()\n",
    "        print((time.time()-to)/60,\"minutes\")\n",
    "            \n",
    "            \n",
    "        torch.save(net.state_dict(),\"/home/jrhoernschemeyer/Desktop/data_prep/egnnfinal\")\n",
    "        torch.save(mha.state_dict(),\"/home/jrhoernschemeyer/Desktop/data_prep/mhafinal\")\n",
    "        torch.save(mha.state_dict(),\"/home/jrhoernschemeyer/Desktop/data_prep/mhafinal\")#nettest\")    \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chararray(['/home/jrhoernschemeyer/Desktop/data_prep/inputs/2ybd.npz',\n",
       "           '/home/jrhoernschemeyer/Desktop/data_prep/inputs/5r2h.npz',\n",
       "           '/home/jrhoernschemeyer/Desktop/data_prep/inputs/3zpg.npz'],\n",
       "          dtype='<U56')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
